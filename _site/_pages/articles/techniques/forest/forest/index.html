<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>Demystifying decision forests - Personal blog</title>
<meta name="description" content="">


  <meta name="author" content="Michele Usuelli">
  


<meta property="og:type" content="website">
<meta property="og:locale" content="en_GB">
<meta property="og:site_name" content="Personal blog">
<meta property="og:title" content="Demystifying decision forests">
<meta property="og:url" content="http://localhost:4000/_pages/articles/techniques/forest/forest/">


  <meta property="og:description" content="">











  

  


<link rel="canonical" href="http://localhost:4000/_pages/articles/techniques/forest/forest/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "Michele Usuelli",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="Personal blog Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--default">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          Personal blog
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/_pages/articles">Articles</a>
            </li><li class="masthead__menu-item">
              <a href="/_pages/books">Books</a>
            </li><li class="masthead__menu-item">
              <a href="/_pages/conferences">Conferences</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <blockquote class="indent markdown-content  ">
      <h1 id="demystifying-decision-forests">Demystifying decision forests</h1>

<p>To build and apply a predictive model, a popular technique is called <em>decision forest</em>. It popularity is due to its versatility and minimality of data preparation requirements.
This article explains the technique without requiring any technical background. By the end of this article, you will get these takeaways</p>
<ul>
  <li>Explanation of the technique</li>
  <li>Interpretation of the results</li>
  <li>Analogies with the human problem-solving approach</li>
</ul>

<p>To demonstrate this, we use as an example a mock use-case based on simulated data. Please be aware that the assumptions behind the example are not necessarily related to reality.</p>

<h2 id="example">Example</h2>

<p>A bank gives loans to its customer. The goal is to understand what customers are trustworthy enough to receive a loan. The bank can use some information related to the customer such as</p>
<ul>
  <li><strong>Credit score</strong>: score measuring the “reliability” of the customer, assessed by a third party</li>
  <li><strong>House age</strong>: age of the house they’re purchasing</li>
  <li><strong>Years of employment</strong></li>
  <li><strong>Credit card debt</strong>: Debt on the credit card</li>
  <li><strong>House year</strong>: Year in which they purchased the house</li>
</ul>

<p>The bank collected historical data about its previous customers. Besides, it traced what customers defaulted via a <strong>default</strong> column. For showing purposes, half of the previous customer defaulted.</p>

<h2 id="decision-tree-explanation">Decision tree explanation</h2>

<table>
  <thead>
    <tr>
      <th>Credit score</th>
      <th>House age</th>
      <th>Years of employment</th>
      <th>Credit card debt</th>
      <th>House year</th>
      <th> </th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>708</td>
      <td>21</td>
      <td>5</td>
      <td>4190</td>
      <td>2002</td>
      <td>0</td>
    </tr>
    <tr>
      <td>649</td>
      <td>20</td>
      <td>3</td>
      <td>2151</td>
      <td>2003</td>
      <td>0</td>
    </tr>
    <tr>
      <td>725</td>
      <td>23</td>
      <td>3</td>
      <td>3603</td>
      <td>2005</td>
      <td>0</td>
    </tr>
    <tr>
      <td>785</td>
      <td>24</td>
      <td>4</td>
      <td>5534</td>
      <td>2009</td>
      <td>0</td>
    </tr>
    <tr>
      <td>649</td>
      <td>14</td>
      <td>6</td>
      <td>5243</td>
      <td>2002</td>
      <td>0</td>
    </tr>
    <tr>
      <td>734</td>
      <td>21</td>
      <td>0</td>
      <td>3441</td>
      <td>2008</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>The <em>decision forest</em> is based on a more simple technique, the <em>decision tree</em>.</p>

<p>Via a statistical metric, we can identify the most default-related information, e.g. the credit card debt. Setting a threshold, we can divide the bank customers into two groups. Then, for each of the two groups, we repeat the process of finding the most relevant feature and we split the customers into two sub-groups.</p>

<p>Ultimately, the tree divides the customers into eights groups for which we compute the number of customers and default percentage.
The following chart shows the tree.
 <img src="https://github.com/micheleusuelli/micheleusuelli.github.io/blob/master/articles-html/forest/tree.png" alt="tree" /></p>

<p>Looking into each split of the three, there are</p>
<ul>
  <li>A number at the top: the propensity of having a default</li>
  <li>A percentage at the bottom: percentage of customers</li>
  <li>Color: the more blue, the more default-propense</li>
</ul>

<p>The percentage is 100% at the top, having all the cusotmers. At the top, we have 0.49 of propensity which means that half of the customers defaulted. Then, the eight groups of customers are sorted by default propensity.</p>

<p>Having a new customer, the bank assigns it to the related groups at the bottom. Let’s have a look at the decision making process, depending on the available customer information</p>

<ol>
  <li>In absence of information, the estimated default risk is the average: 50%</li>
  <li>Knowing that the credit card debt is below $6755, the customer has a risk of 6.6%</li>
  <li>Knowing that the credit card debt is above $6755, the customer has a risk of 56%</li>
  <li>Having the complete information about the customer, it gets assigned to one of the 8 groups at the bottom of the tree</li>
</ol>

<h2 id="results-interpretation">Results interpretation</h2>

<p>To interpret the results, let’s look further into the decision-making process:</p>

<ul>
  <li>if below ccDebt &lt; 6755, the score becomes 6.6%, so it gets decreased by 42.4%</li>
  <li>if above ccDebt &lt;= 6755, the score becomes 82%, so it gets increased by 33%</li>
</ul>

<p>Summarising, this is the impact:</p>

<ul>
  <li>ccDebt &lt; threshold: -0.424</li>
  <li>ccDebt &lt;= threshold: +0.33</li>
</ul>

<p>In the same way, we can measure the impact of being above or below the threshold of any other piece of information. If the same information is being used more than once, we can simply sum up the effects.</p>

<p>Depending on the customer, the path on the tree is different, so the features being used and their impact changes. To keep track of that, we can sum up the effects separately for each customer and visualise them through a decision-making heatmap:</p>

<ul>
  <li>A row per customer</li>
  <li>A column per piece of information either above or below a generic threshold</li>
  <li>A blue cell if the information is decreasing the default propensity</li>
  <li>A red cell if the information is increasing the default propensity</li>
  <li>A stronger shade of either red or blue depending on the impact</li>
</ul>

<p><img src="https://github.com/micheleusuelli/micheleusuelli.github.io/blob/master/articles-html/forest/heatmap.png" alt=" heatmap" /></p>

<p>Via the heatmap, the bank can easily show the rational behind each prediction and therefore interpret the results.</p>

<h2 id="from-the-tree-to-the-forest">From the tree to the forest</h2>

<p>So far, we have seen an explanation of the decision tree and an interpretation of its results. As stated initially, this article focuses on decision forests. As suggested by its name, a decision forest is a combination of decision trees.</p>

<p>Let’s imagine that 100 agents access different parts of the customer information</p>
<ul>
  <li>A different random group of customer</li>
  <li>A different random subset of the information</li>
</ul>

<p>Asking each agent to build a decision tree, we will have 100 different trees and therefore 100 different outcomes. Combining them, the final result will take account of different perspectives that would be hidden building a single decision tree. This methodology, called “ensembling”, usually increases the performance of the predictive model.</p>

<p>However, the methodology described above still applies. The final default propensity can simply be the average. Simply, we can sum-up the effect of each piece of information across all the trees of the forest and build a similar heatmap. The results will be interpretable in the same way as the decision tree.</p>

<h2 id="conclusions">Conclusions</h2>

<p>Coming back to the key takeaways:</p>
<ul>
  <li>Explanation of the technique: the decision-making process can be visualized and explained</li>
  <li>Interpretation of the results: the rationale behing the results can be interpreted and visualized</li>
  <li>Analogies with the human problem-solving approach: the solution if based on a series of yes/no answers, similarly to a human decision tree</li>
</ul>

<p>Likewise, the same approach is applicable to other contexts.</p>

    </blockquote>

    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
          <li><a href="https://www.linkedin.com/in/michele-usuelli-1b84b460/" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i> LinkedIn</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2023 Michele Usuelli. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>










  </body>
</html>
